{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a follow-up to my previous post regarding scrapping using Selenium and Chromedriver to extact data from the web. When I first started out with Python my code was just a huge block of text. The concept of a class was far and away from my ability to comprehend at the time. I'm sure this can be the case with anyone that starts out learning a programming language. But after some experience and learning how classes work, I decided it was high time to rebuild my scraping project with a `Scraper` object. You can check out [my github public repo](https://github.com/virrey/sandbox_public/) for the `Scraper` class I created which is used in this demo and leverages [Selenium Webdriver](https://www.seleniumhq.org/projects/webdriver/). Having created this class, I noticed the my code was much cleaner and easier to follow and debug. Let us dive in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we import my `Scraper` class and the `robotparser`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ariScrape import Scraper\n",
    "from urllib import robotparser "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will again respectfully check the sites `robots.txt` file to ensure we are being considerate of the sites scrapping policies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def robot_check(url_parts):\n",
    "    url = ''.join([str(p) for p in url_parts])\n",
    "    rp = robotparser.RobotFileParser()\n",
    "    rp.set_url(url)\n",
    "    rp.read()\n",
    "    if rp.can_fetch(\"*\", url):\n",
    "        print(\"Robots.txt: User Allowed\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"Robots.txt: User Disallowed. Please abort.\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we'll be scraping the Intel Corp Income Statement from the [NASDAQ](www.nasdaq.com/symbol/INTC) website. We'll call the `robots_check` function to confirm we are good to go on the scraping side. You will notice here I split the url into 3 parts. From my previous projects it was important to be able to frame urls this way because when you are looping through stock tickers only part of the url will change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Robots.txt: User Allowed\n"
     ]
    }
   ],
   "source": [
    "# test ticker symbol @ nasdaq.com\n",
    "test_ticker = \"INTC\"\n",
    "url_prefix = \"https://www.nasdaq.com/symbol/\"\n",
    "url_suffix = \"/financials\"\n",
    "url_parts = [url_prefix,test_ticker,url_suffix]\n",
    "robot_check(url_parts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My `Scraper` class also allows defining parts of the xpath. The reason for this will be obvious in the next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xpath of @top left of table @url\n",
    "# >> //*[@id=\"financials-iframe-wrap\"]/div[1]/table/tbody/tr[1]/td[2]\n",
    "# xpath of @bottom right of table @url\n",
    "# >> //*[@id=\"financials-iframe-wrap\"]/div[1]/table/tbody/tr[19]/td[5]\n",
    "# these are the string parts of the xpath that\n",
    "# will not change while we iterate\n",
    "xpath_part1 = '//*[@id=\"financials-iframe-wrap\"]/div[1]/table/tbody/tr['\n",
    "xpath_part2 = ']/td['\n",
    "xpath_part3 = ']'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you go into the inspector on Chrome you will notice the xpath of the income statement items are effectively a table of rows and columns. Therefore, our `xpath_column_key` and `xpath_row_key` are an enumeration of those columns and rows. You can then see why the `xpath_parts` assignment is perfect for our needs. We create a list of `xpath_parts` list in a double for loop list comprehension by iterating over our column and row keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init the column and row keys for the example table\n",
    "xpath_column_key = [i for i in range(2,6,1)] # create column index 2-5\n",
    "xpath_row_key = [i for i in range(1,20,1)] # create row index 1-19\n",
    "\n",
    "# make an array of xpath_parts that we're going to iterate over\n",
    "xpath_parts = [[xpath_part1,r,xpath_part2,c,xpath_part3] for c in xpath_column_key for r in xpath_row_key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the `url_parts` and our list of `xpath_parts` we are ready to create our `Scraper` object. Set the scraper object property to True if you would like to see all the internal debugging statements from the class. Otherwise, leave this as False. `getChromeDriverPath` will take a path that is user defined or will look for the chromedriver in some default locations. It also checks if you are on Mac or Win. Once the driver is created, set the URL to be scrapped using `setURLFromParts` then `LoadPage`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an ariScrape object\n",
    "scrapeObj = Scraper()\n",
    "scrapeObj.verbose = False\n",
    "scrapeObj.getChromeDriverPath()\n",
    "scrapeObj.getChromeDriver()\n",
    "scrapeObj.setURLFromParts(url_parts)\n",
    "scrapeObj.LoadPage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chrome should now open in test mode. If you would like the values being extracted to show leave `val_verbose` set to True. The loop iterates through our `xpath_parts` list to go through each of the column/row combinations and extract the value. Then we close the webdriver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set this to True if you want to see the extracted\n",
    "# values -- useful when obj.verbose is False\n",
    "val_verbose = True\n",
    "for xp in xpath_parts:\n",
    "    scrapeObj.setXPathFromParts(xp)\n",
    "    scrapeObj.ExtractFloatAtXPath()\n",
    "    if val_verbose:\n",
    "        print(scrapeObj.scrapped_item)\n",
    "scrapeObj.driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have any recommendations to improving the code please feel free to reach out. I'm always seeking to learn new and better ways of coding. I hope you enjoyed reading, and remember:\n",
    "\n",
    "Stay Chaotic â€“ Stay Neutral\n",
    "\n",
    "[ARI](mailto:ari.virrey@gmail.com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
